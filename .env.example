# LLM Environment variables

# Reasoning LLM (for complex reasoning tasks)
# If you're using your local Ollama, replace the model name after the slash and base url then you're good to go.
# For wider model support, read https://docs.litellm.ai/docs/providers.
# REASONING_API_KEY=
# REASONING_BASE_URL=
#REASONING_MODEL=qwq-plus
# REASONING_MODEL=

# Non-reasoning LLM (for straightforward tasks)
# BASIC_API_KEY=
# BASIC_BASE_URL=
# BASIC_MODEL=
#BASIC_MODEL=qwen-max-latest

# CODE_API_KEY=
# CODE_BASE_URL=
# CODE_MODEL=
# CODE_MODEL=deepseek-chat

# Generate_avatar_API_KEY=
# Generate_avatar_BASE_URL=
# Generate_avatar_MODEL=

# VIDEO_MODEL=
# Vision-language LLM (for tasks requiring visual understanding)
# VL_API_KEY=
# VL_BASE_URL=
# VL_MODEL=
#VL_MODEL=qwen2.5-vl-72b-instruct

# Application Settings
# DEBUG=True
# APP_ENV=development

# Add other environment variables as needed
# TAVILY_API_KEY=
# JINA_API_KEY= # Optional, default is None

# CHROME_INSTANCE_PATH=/Applications/Google Chrome.app/Contents/MacOS/Google Chrome
# CHROME_HEADLESS=False  # Optional, default is False
# CHROME_PROXY_SERVER=http://127.0.0.1:10809  # Optional, default is None
# CHROME_PROXY_USERNAME=  # Optional, default is None
# CHROME_PROXY_PASSWORD=  # Optional, default is None


# turn off for collecting anonymous usage information
# ANONYMIZED_TELEMETRY=

# SLACK_USER_TOKEN=

# SILICONFLOW_API_KEY=